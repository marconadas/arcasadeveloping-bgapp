#!/usr/bin/env python3
"""
Sistema de Backup Robusto e Disaster Recovery
Garante 99.99% disponibilidade dos dados
"""

import os
import json
import shutil
import tarfile
import asyncio
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum

import psutil
import boto3
from pydantic import BaseModel

class BackupType(str, Enum):
    """Tipos de backup"""
    FULL = "full"
    INCREMENTAL = "incremental" 
    DIFFERENTIAL = "differential"
    CONFIG = "config"

class BackupStatus(str, Enum):
    """Status do backup"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    RESTORED = "restored"

@dataclass
class BackupJob:
    """Job de backup"""
    id: str
    type: BackupType
    status: BackupStatus
    source: str
    destination: str
    size_mb: float
    started_at: datetime
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    
    def to_dict(self) -> Dict:
        data = asdict(self)
        data['started_at'] = self.started_at.isoformat()
        if self.completed_at:
            data['completed_at'] = self.completed_at.isoformat()
        return data

class BackupConfig(BaseModel):
    """Configura√ß√£o do sistema de backup"""
    backup_dir: str = "/app/backups"
    s3_bucket: Optional[str] = "bgapp-backups"
    retention_days: int = 30
    max_backup_size_gb: float = 10.0
    compression: bool = True
    encryption: bool = True
    schedule_full: str = "0 2 * * 0"  # Domingo √†s 2h
    schedule_incremental: str = "0 3 * * 1-6"  # Segunda a S√°bado √†s 3h

class BackupManager:
    """Gerenciador de backup e disaster recovery"""
    
    def __init__(self, config: BackupConfig = None):
        self.config = config or BackupConfig()
        self.backup_jobs: List[BackupJob] = []
        self.backup_dir = Path(self.config.backup_dir)
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # Configurar AWS S3 (opcional)
        self.s3_client = None
        if self.config.s3_bucket:
            try:
                self.s3_client = boto3.client('s3')
            except Exception as e:
                print(f"‚ö†Ô∏è S3 n√£o configurado: {e}")
    
    async def create_database_backup(self, backup_type: BackupType = BackupType.FULL) -> BackupJob:
        """Criar backup da base de dados PostgreSQL"""
        job_id = f"db_{backup_type}_{int(datetime.now().timestamp())}"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = self.backup_dir / f"database_{backup_type}_{timestamp}.sql"
        
        job = BackupJob(
            id=job_id,
            type=backup_type,
            status=BackupStatus.PENDING,
            source="postgresql://postgres:postgres@localhost:5432/geo",
            destination=str(backup_file),
            size_mb=0.0,
            started_at=datetime.now()
        )
        
        self.backup_jobs.append(job)
        
        try:
            job.status = BackupStatus.RUNNING
            print(f"üóÑÔ∏è Iniciando backup {backup_type} da base de dados...")
            
            # Comando pg_dump
            cmd = [
                "docker", "exec", "infra-postgis-1",
                "pg_dump", "-U", "postgres", "-d", "geo",
                "--no-password", "--verbose"
            ]
            
            if backup_type == BackupType.FULL:
                cmd.extend(["--clean", "--create"])
            elif backup_type == BackupType.INCREMENTAL:
                # Para incremental, usar WAL shipping (simplificado aqui)
                cmd.extend(["--format=custom"])
                
            # Executar backup
            with open(backup_file, 'w') as f:
                result = subprocess.run(
                    cmd, stdout=f, stderr=subprocess.PIPE, 
                    text=True, timeout=3600  # 1 hora timeout
                )
                
            if result.returncode == 0:
                # Comprimir se configurado
                if self.config.compression:
                    await self._compress_file(backup_file)
                    
                # Calcular tamanho
                job.size_mb = backup_file.stat().st_size / (1024 * 1024)
                
                # Upload para S3 se configurado
                if self.s3_client:
                    await self._upload_to_s3(backup_file, f"database/{backup_file.name}")
                
                job.status = BackupStatus.COMPLETED
                job.completed_at = datetime.now()
                
                print(f"‚úÖ Backup da base de dados conclu√≠do: {job.size_mb:.1f}MB")
                
            else:
                job.status = BackupStatus.FAILED
                job.error_message = result.stderr
                print(f"‚ùå Erro no backup da base de dados: {result.stderr}")
                
        except Exception as e:
            job.status = BackupStatus.FAILED
            job.error_message = str(e)
            print(f"‚ùå Erro no backup da base de dados: {e}")
            
        return job
    
    async def create_files_backup(self, source_paths: List[str]) -> BackupJob:
        """Criar backup de arquivos e configura√ß√µes"""
        job_id = f"files_{int(datetime.now().timestamp())}"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = self.backup_dir / f"files_{timestamp}.tar.gz"
        
        job = BackupJob(
            id=job_id,
            type=BackupType.CONFIG,
            status=BackupStatus.PENDING,
            source=",".join(source_paths),
            destination=str(backup_file),
            size_mb=0.0,
            started_at=datetime.now()
        )
        
        self.backup_jobs.append(job)
        
        try:
            job.status = BackupStatus.RUNNING
            print(f"üìÅ Iniciando backup de arquivos...")
            
            # Criar arquivo tar comprimido
            with tarfile.open(backup_file, "w:gz") as tar:
                for source_path in source_paths:
                    if os.path.exists(source_path):
                        tar.add(source_path, arcname=os.path.basename(source_path))
                        print(f"  ‚úì Adicionado: {source_path}")
                        
            # Calcular tamanho
            job.size_mb = backup_file.stat().st_size / (1024 * 1024)
            
            # Upload para S3 se configurado
            if self.s3_client:
                await self._upload_to_s3(backup_file, f"files/{backup_file.name}")
                
            job.status = BackupStatus.COMPLETED
            job.completed_at = datetime.now()
            
            print(f"‚úÖ Backup de arquivos conclu√≠do: {job.size_mb:.1f}MB")
            
        except Exception as e:
            job.status = BackupStatus.FAILED
            job.error_message = str(e)
            print(f"‚ùå Erro no backup de arquivos: {e}")
            
        return job
    
    async def create_minio_backup(self) -> BackupJob:
        """Criar backup dos dados MinIO"""
        job_id = f"minio_{int(datetime.now().timestamp())}"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = self.backup_dir / f"minio_{timestamp}.tar.gz"
        
        job = BackupJob(
            id=job_id,
            type=BackupType.FULL,
            status=BackupStatus.PENDING,
            source="minio-data",
            destination=str(backup_file),
            size_mb=0.0,
            started_at=datetime.now()
        )
        
        self.backup_jobs.append(job)
        
        try:
            job.status = BackupStatus.RUNNING
            print(f"ü™£ Iniciando backup do MinIO...")
            
            # Copiar dados do volume Docker MinIO
            minio_volume_path = self._get_docker_volume_path("infra_minio-data")
            
            if minio_volume_path and os.path.exists(minio_volume_path):
                with tarfile.open(backup_file, "w:gz") as tar:
                    tar.add(minio_volume_path, arcname="minio-data")
                    
                job.size_mb = backup_file.stat().st_size / (1024 * 1024)
                
                # Upload para S3 se configurado (e n√£o for o pr√≥prio S3!)
                if self.s3_client and self.config.s3_bucket:
                    await self._upload_to_s3(backup_file, f"minio/{backup_file.name}")
                    
                job.status = BackupStatus.COMPLETED
                job.completed_at = datetime.now()
                
                print(f"‚úÖ Backup do MinIO conclu√≠do: {job.size_mb:.1f}MB")
                
            else:
                job.status = BackupStatus.FAILED
                job.error_message = "Volume MinIO n√£o encontrado"
                print(f"‚ùå Volume MinIO n√£o encontrado")
                
        except Exception as e:
            job.status = BackupStatus.FAILED
            job.error_message = str(e)
            print(f"‚ùå Erro no backup do MinIO: {e}")
            
        return job
    
    async def create_full_system_backup(self) -> List[BackupJob]:
        """Criar backup completo do sistema"""
        print("üöÄ Iniciando backup completo do sistema BGAPP...")
        
        jobs = []
        
        # 1. Backup da base de dados
        db_job = await self.create_database_backup(BackupType.FULL)
        jobs.append(db_job)
        
        # 2. Backup dos arquivos de configura√ß√£o
        config_paths = [
            "/app/configs",
            "/app/infra",
            "/app/scripts",
            "/app/logs"
        ]
        files_job = await self.create_files_backup(config_paths)
        jobs.append(files_job)
        
        # 3. Backup do MinIO
        minio_job = await self.create_minio_backup()
        jobs.append(minio_job)
        
        # Estat√≠sticas finais
        total_size = sum(job.size_mb for job in jobs if job.status == BackupStatus.COMPLETED)
        completed_jobs = len([job for job in jobs if job.status == BackupStatus.COMPLETED])
        
        print(f"üìä Backup completo finalizado:")
        print(f"   ‚úÖ Jobs conclu√≠dos: {completed_jobs}/{len(jobs)}")
        print(f"   üì¶ Tamanho total: {total_size:.1f}MB")
        
        return jobs
    
    async def restore_database_backup(self, backup_file: str) -> bool:
        """Restaurar backup da base de dados"""
        try:
            print(f"üîÑ Restaurando base de dados de {backup_file}...")
            
            # Verificar se arquivo existe
            backup_path = Path(backup_file)
            if not backup_path.exists():
                print(f"‚ùå Arquivo de backup n√£o encontrado: {backup_file}")
                return False
                
            # Descomprimir se necess√°rio
            if backup_file.endswith('.gz'):
                await self._decompress_file(backup_path)
                backup_file = str(backup_path).replace('.gz', '')
                
            # Comando psql para restaurar
            cmd = [
                "docker", "exec", "-i", "infra-postgis-1",
                "psql", "-U", "postgres", "-d", "geo"
            ]
            
            with open(backup_file, 'r') as f:
                result = subprocess.run(
                    cmd, stdin=f, stderr=subprocess.PIPE,
                    text=True, timeout=3600
                )
                
            if result.returncode == 0:
                print("‚úÖ Base de dados restaurada com sucesso!")
                return True
            else:
                print(f"‚ùå Erro restaurando base de dados: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"‚ùå Erro restaurando base de dados: {e}")
            return False
    
    async def cleanup_old_backups(self):
        """Limpar backups antigos baseado na reten√ß√£o configurada"""
        cutoff_date = datetime.now() - timedelta(days=self.config.retention_days)
        
        cleaned_count = 0
        cleaned_size = 0
        
        for backup_file in self.backup_dir.glob("*"):
            if backup_file.is_file():
                file_time = datetime.fromtimestamp(backup_file.stat().st_mtime)
                if file_time < cutoff_date:
                    file_size = backup_file.stat().st_size
                    backup_file.unlink()
                    cleaned_count += 1
                    cleaned_size += file_size
                    
        print(f"üßπ Limpeza de backups: {cleaned_count} arquivos removidos ({cleaned_size/(1024*1024):.1f}MB)")
    
    def _get_docker_volume_path(self, volume_name: str) -> Optional[str]:
        """Obter caminho do volume Docker"""
        try:
            result = subprocess.run([
                "docker", "volume", "inspect", volume_name,
                "--format", "{{.Mountpoint}}"
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                return result.stdout.strip()
            return None
            
        except Exception:
            return None
    
    async def _compress_file(self, file_path: Path):
        """Comprimir arquivo"""
        try:
            with open(file_path, 'rb') as f_in:
                with tarfile.open(f"{file_path}.gz", 'w:gz') as tar:
                    tarinfo = tarfile.TarInfo(name=file_path.name)
                    tarinfo.size = file_path.stat().st_size
                    tar.addfile(tarinfo, f_in)
                    
            # Remover arquivo original
            file_path.unlink()
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro comprimindo {file_path}: {e}")
    
    async def _decompress_file(self, file_path: Path):
        """Descomprimir arquivo"""
        try:
            with tarfile.open(file_path, 'r:gz') as tar:
                tar.extractall(path=file_path.parent)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erro descomprimindo {file_path}: {e}")
    
    async def _upload_to_s3(self, file_path: Path, s3_key: str):
        """Upload para S3"""
        try:
            if self.s3_client:
                self.s3_client.upload_file(
                    str(file_path), 
                    self.config.s3_bucket, 
                    s3_key
                )
                print(f"‚òÅÔ∏è Upload para S3: {s3_key}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erro upload S3: {e}")
    
    async def get_backup_dashboard(self) -> Dict:
        """Obter dashboard de backups"""
        total_backups = len(self.backup_jobs)
        successful_backups = len([job for job in self.backup_jobs if job.status == BackupStatus.COMPLETED])
        failed_backups = len([job for job in self.backup_jobs if job.status == BackupStatus.FAILED])
        
        total_size = sum(job.size_mb for job in self.backup_jobs if job.status == BackupStatus.COMPLETED)
        
        # √öltimo backup bem-sucedido
        last_successful = None
        for job in reversed(self.backup_jobs):
            if job.status == BackupStatus.COMPLETED:
                last_successful = job
                break
        
        # Espa√ßo dispon√≠vel
        disk_usage = psutil.disk_usage(str(self.backup_dir))
        available_gb = disk_usage.free / (1024**3)
        
        return {
            "summary": {
                "total_backups": total_backups,
                "successful_backups": successful_backups,
                "failed_backups": failed_backups,
                "success_rate": (successful_backups / total_backups * 100) if total_backups > 0 else 0
            },
            "storage": {
                "total_size_mb": total_size,
                "available_space_gb": available_gb,
                "backup_directory": str(self.backup_dir)
            },
            "last_backup": last_successful.to_dict() if last_successful else None,
            "recent_jobs": [job.to_dict() for job in self.backup_jobs[-10:]],  # √öltimos 10
            "configuration": {
                "retention_days": self.config.retention_days,
                "compression": self.config.compression,
                "s3_enabled": self.s3_client is not None
            }
        }
    
    async def schedule_automated_backups(self):
        """Agendar backups autom√°ticos"""
        print("‚è∞ Iniciando sistema de backups autom√°ticos...")
        
        while True:
            try:
                current_time = datetime.now()
                
                # Verificar se √© hora do backup completo (domingo 2h)
                if (current_time.weekday() == 6 and  # Domingo
                    current_time.hour == 2 and
                    current_time.minute == 0):
                    
                    await self.create_full_system_backup()
                
                # Verificar se √© hora do backup incremental (segunda a s√°bado 3h)
                elif (current_time.weekday() < 6 and  # Segunda a S√°bado
                      current_time.hour == 3 and
                      current_time.minute == 0):
                    
                    await self.create_database_backup(BackupType.INCREMENTAL)
                
                # Limpeza di√°ria √†s 4h
                elif current_time.hour == 4 and current_time.minute == 0:
                    await self.cleanup_old_backups()
                
                # Aguardar 1 minuto antes da pr√≥xima verifica√ß√£o
                await asyncio.sleep(60)
                
            except Exception as e:
                print(f"‚ùå Erro no scheduler de backups: {e}")
                await asyncio.sleep(300)  # 5 minutos em caso de erro

# Inst√¢ncia global do gerenciador de backup
backup_manager = BackupManager()

async def start_backup_scheduler():
    """Iniciar scheduler de backups"""
    await backup_manager.schedule_automated_backups()

if __name__ == "__main__":
    # Executar scheduler de backups
    asyncio.run(start_backup_scheduler())
