#!/usr/bin/env python3
"""
ğŸ§ª ML Retention Performance Tests
Testes para validar ganhos de performance da base de retenÃ§Ã£o ML

OBJETIVOS:
- Validar que o sistema nÃ£o afeta funcionalidade existente
- Medir ganhos reais de performance
- Testar compatibilidade com Cloudflare
- Verificar polÃ­ticas de retenÃ§Ã£o
"""

import asyncio
import json
import logging
import time
import statistics
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple
import sys
import os

# Adicionar path do projeto
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Imports do sistema de retenÃ§Ã£o
try:
    from src.bgapp.ml.retention_manager import MLRetentionManager, create_retention_manager
    from src.bgapp.ml.retention_pipeline import MLRetentionPipeline, get_retention_pipeline
    from src.bgapp.ml.retention_policies import MLRetentionPolicyManager, create_policy_manager
    from src.bgapp.ml.retention_integration import MLRetentionIntegrator, initialize_ml_retention_system
    from src.bgapp.ml.retention_monitoring import MLRetentionMonitor, get_retention_monitor
except ImportError as e:
    print(f"âŒ Erro importando mÃ³dulos: {e}")
    print("Execute este teste a partir do diretÃ³rio raiz do projeto")
    sys.exit(1)

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PerformanceTestSuite:
    """
    ğŸ§ª Suite de Testes de Performance
    
    Testa todos os componentes do sistema de retenÃ§Ã£o ML
    e valida ganhos de performance sem afetar funcionalidade.
    """
    
    def __init__(self):
        """Inicializar suite de testes"""
        
        self.test_results = {}
        self.performance_metrics = {}
        self.errors = []
        
        # ConfiguraÃ§Ãµes de teste
        self.test_config = {
            'test_iterations': 100,
            'cache_test_size': 50,
            'feature_test_size': 20,
            'timeout_seconds': 30
        }
        
        logger.info("ğŸ§ª Performance Test Suite inicializada")
    
    # =====================================
    # ğŸ§ª TESTES DE FUNCIONALIDADE
    # =====================================
    
    async def test_retention_manager_basic(self) -> Dict[str, Any]:
        """Testar funcionalidade bÃ¡sica do retention manager"""
        
        logger.info("ğŸ”§ Testando Retention Manager...")
        
        try:
            # Criar manager em modo readonly para testes
            manager = create_retention_manager(readonly_mode=True)
            
            # Testar stats
            stats = manager.get_lightweight_stats()
            assert isinstance(stats, dict), "Stats devem ser um dict"
            assert 'hit_ratio' in stats, "Stats devem incluir hit_ratio"
            
            # Testar cache em memÃ³ria
            test_key = "test_feature_123"
            test_data = {"temperature": 25.5, "depth": 100}
            
            # Simular cache hit
            manager.memory_cache[manager.CacheType.FEATURE_STORE][test_key] = test_data
            
            # Verificar cache
            cached_data = manager.memory_cache[manager.CacheType.FEATURE_STORE].get(test_key)
            assert cached_data == test_data, "Cache em memÃ³ria deve funcionar"
            
            return {
                'status': 'passed',
                'stats': stats,
                'cache_test': 'passed'
            }
            
        except Exception as e:
            logger.error(f"âŒ Erro no teste do retention manager: {e}")
            self.errors.append(f"retention_manager_basic: {str(e)}")
            return {'status': 'failed', 'error': str(e)}
    
    async def test_pipeline_processing(self) -> Dict[str, Any]:
        """Testar pipeline de processamento"""
        
        logger.info("ğŸ”„ Testando Pipeline...")
        
        try:
            pipeline = get_retention_pipeline()
            
            # Dados de teste
            test_study = {
                'study_id': 'test_study_001',
                'latitude': -12.5,
                'longitude': 18.3,
                'start_date': '2024-08-15T10:00:00Z',
                'environmental_parameters': {
                    'temperature_mean': 22.5,
                    'chlorophyll_mean': 3.2,
                    'salinity_mean': 35.1
                },
                'species_observed': [
                    {'species_name': 'Sardinella aurita', 'count': 15},
                    {'species_name': 'Trachurus capensis', 'count': 8}
                ]
            }
            
            # Processar estudo
            start_time = time.time()
            await pipeline.process_new_study(test_study)
            processing_time = (time.time() - start_time) * 1000
            
            # Verificar mÃ©tricas
            metrics = pipeline.get_pipeline_metrics()
            
            return {
                'status': 'passed',
                'processing_time_ms': processing_time,
                'metrics': metrics
            }
            
        except Exception as e:
            logger.error(f"âŒ Erro no teste do pipeline: {e}")
            self.errors.append(f"pipeline_processing: {str(e)}")
            return {'status': 'failed', 'error': str(e)}
    
    async def test_policy_manager(self) -> Dict[str, Any]:
        """Testar gestor de polÃ­ticas"""
        
        logger.info("ğŸ“‹ Testando Policy Manager...")
        
        try:
            policy_manager = create_policy_manager(auto_start=False)
            
            # Carregar polÃ­ticas padrÃ£o
            await policy_manager.load_default_policies()
            
            # Verificar polÃ­ticas carregadas
            status = policy_manager.get_policy_status()
            assert status['total_policies'] > 0, "Deve ter polÃ­ticas carregadas"
            
            # Testar execuÃ§Ã£o dry-run
            results = await policy_manager.execute_all_policies(dry_run=True)
            
            return {
                'status': 'passed',
                'total_policies': status['total_policies'],
                'enabled_policies': status['enabled_policies'],
                'dry_run_results': len(results)
            }
            
        except Exception as e:
            logger.error(f"âŒ Erro no teste do policy manager: {e}")
            self.errors.append(f"policy_manager: {str(e)}")
            return {'status': 'failed', 'error': str(e)}
    
    async def test_monitoring_system(self) -> Dict[str, Any]:
        """Testar sistema de monitorizaÃ§Ã£o"""
        
        logger.info("ğŸ“Š Testando Monitoring System...")
        
        try:
            monitor = get_retention_monitor()
            
            # Coletar mÃ©tricas
            metrics = await monitor.collect_current_metrics()
            
            # Gerar relatÃ³rio
            report = await monitor.generate_performance_report(period_hours=1)
            
            # Verificar dashboard
            dashboard_data = monitor.get_dashboard_data()
            
            # Verificar health status
            health_status = await monitor.get_health_status()
            
            return {
                'status': 'passed',
                'metrics_collected': len(metrics),
                'report_generated': report.report_id,
                'dashboard_data': bool(dashboard_data),
                'health_status': health_status['overall_status']
            }
            
        except Exception as e:
            logger.error(f"âŒ Erro no teste do monitoring: {e}")
            self.errors.append(f"monitoring_system: {str(e)}")
            return {'status': 'failed', 'error': str(e)}
    
    # =====================================
    # âš¡ TESTES DE PERFORMANCE
    # =====================================
    
    async def test_cache_performance(self) -> Dict[str, Any]:
        """Testar performance do cache"""
        
        logger.info("âš¡ Testando Performance do Cache...")
        
        try:
            manager = create_retention_manager(readonly_mode=True)
            
            # Preparar dados de teste
            test_features = []
            for i in range(self.test_config['cache_test_size']):
                test_features.append({
                    'source_id': f'test_{i}',
                    'features': {
                        'temperature': 20 + (i * 0.5),
                        'depth': 50 + i,
                        'salinity': 35 + (i * 0.1)
                    }
                })
            
            # Teste sem cache (simulado)
            start_time = time.time()
            for feature_data in test_features:
                # Simular processamento sem cache
                await asyncio.sleep(0.01)  # 10ms por operaÃ§Ã£o
            no_cache_time = time.time() - start_time
            
            # Teste com cache (simulado)
            start_time = time.time()
            for i, feature_data in enumerate(test_features):
                cache_key = f"test_cache_{i}"
                # Simular cache hit (muito mais rÃ¡pido)
                manager.memory_cache[manager.CacheType.FEATURE_STORE][cache_key] = feature_data['features']
                cached_data = manager.memory_cache[manager.CacheType.FEATURE_STORE].get(cache_key)
                assert cached_data is not None
            cache_time = time.time() - start_time
            
            # Calcular speedup
            speedup = no_cache_time / cache_time if cache_time > 0 else float('inf')
            
            return {
                'status': 'passed',
                'no_cache_time_ms': no_cache_time * 1000,
                'cache_time_ms': cache_time * 1000,
                'speedup_factor': speedup,
                'test_size': self.test_config['cache_test_size']
            }
            
        except Exception as e:
            logger.error(f"âŒ Erro no teste de performance do cache: {e}")
            self.errors.append(f"cache_performance: {str(e)}")
            return {'status': 'failed', 'error': str(e)}
    
    async def test_feature_extraction_performance(self) -> Dict[str, Any]:
        """Testar performance da extraÃ§Ã£o de caracterÃ­sticas"""
        
        logger.info("ğŸ” Testando Performance da ExtraÃ§Ã£o de Features...")
        
        try:
            pipeline = get_retention_pipeline()
            
            # Dados de teste
            test_studies = []
            for i in range(self.test_config['feature_test_size']):
                test_studies.append({
                    'study_id': f'perf_test_{i}',
                    'latitude': -12.5 + (i * 0.1),
                    'longitude': 18.3 + (i * 0.1),
                    'start_date': f'2024-08-{15 + (i % 15):02d}T10:00:00Z',
                    'environmental_parameters': {
                        'temperature_mean': 22.5 + i,
                        'chlorophyll_mean': 3.2 + (i * 0.1)
                    },
                    'species_observed': [
                        {'species_name': f'Species_{i}', 'count': 10 + i}
                    ]
                })
            
            # Testar extraÃ§Ã£o de caracterÃ­sticas
            extraction_times = []
            
            for study in test_studies:
                start_time = time.time()
                
                # Extrair caracterÃ­sticas temporais
                temporal_features = await pipeline._extract_temporal_features(
                    study['study_id'], study
                )
                
                # Extrair caracterÃ­sticas espaciais
                spatial_features = await pipeline._extract_spatial_features(
                    study['study_id'], study
                )
                
                # Extrair caracterÃ­sticas ambientais
                if study.get('environmental_parameters'):
                    env_features = await pipeline._extract_environmental_features(
                        study['study_id'], study
                    )
                
                extraction_time = (time.time() - start_time) * 1000
                extraction_times.append(extraction_time)
            
            # Calcular estatÃ­sticas
            avg_extraction_time = statistics.mean(extraction_times)
            min_extraction_time = min(extraction_times)
            max_extraction_time = max(extraction_times)
            
            return {
                'status': 'passed',
                'avg_extraction_time_ms': avg_extraction_time,
                'min_extraction_time_ms': min_extraction_time,
                'max_extraction_time_ms': max_extraction_time,
                'total_studies_processed': len(test_studies),
                'features_per_second': 1000 / avg_extraction_time if avg_extraction_time > 0 else float('inf')
            }
            
        except Exception as e:
            logger.error(f"âŒ Erro no teste de performance de features: {e}")
            self.errors.append(f"feature_extraction_performance: {str(e)}")
            return {'status': 'failed', 'error': str(e)}
    
    async def test_integration_performance(self) -> Dict[str, Any]:
        """Testar performance da integraÃ§Ã£o completa"""
        
        logger.info("ğŸ”— Testando Performance da IntegraÃ§Ã£o...")
        
        try:
            # Inicializar sistema sem modificar APIs existentes
            integrator = initialize_ml_retention_system(
                enable_integration=False,  # NÃ£o modificar sistema em teste
                cloudflare_mode=False,
                auto_start=False
            )
            
            # Testar mÃ©tricas de integraÃ§Ã£o
            integration_metrics = integrator.get_integration_metrics()
            
            # Testar health check
            health_status = await integrator.health_check()
            
            return {
                'status': 'passed',
                'integration_enabled': integration_metrics['integration_enabled'],
                'cloudflare_mode': integration_metrics['cloudflare_mode'],
                'health_status': health_status['overall_status'],
                'cache_hit_ratio': integration_metrics.get('cache_hit_ratio', 0.0)
            }
            
        except Exception as e:
            logger.error(f"âŒ Erro no teste de performance de integraÃ§Ã£o: {e}")
            self.errors.append(f"integration_performance: {str(e)}")
            return {'status': 'failed', 'error': str(e)}
    
    # =====================================
    # â˜ï¸ TESTES CLOUDFLARE
    # =====================================
    
    async def test_cloudflare_compatibility(self) -> Dict[str, Any]:
        """Testar compatibilidade com Cloudflare Workers"""
        
        logger.info("â˜ï¸ Testando Compatibilidade Cloudflare...")
        
        try:
            # Criar sistema em modo Cloudflare
            integrator = initialize_ml_retention_system(
                enable_integration=False,
                cloudflare_mode=True,
                auto_start=False
            )
            
            # Verificar modo Cloudflare
            assert integrator.cloudflare_mode == True, "Modo Cloudflare deve estar ativo"
            assert integrator.retention_manager.readonly_mode == True, "Deve estar em modo readonly"
            
            # Testar middleware para workers
            middleware = integrator.create_worker_middleware()
            assert callable(middleware), "Middleware deve ser uma funÃ§Ã£o"
            
            # Testar stats leves
            stats = integrator.retention_manager.get_lightweight_stats()
            assert 'status' in stats, "Stats devem incluir status"
            assert stats['status'] == 'readonly', "Status deve ser readonly"
            
            return {
                'status': 'passed',
                'cloudflare_mode': True,
                'readonly_mode': True,
                'middleware_created': True,
                'lightweight_stats': stats
            }
            
        except Exception as e:
            logger.error(f"âŒ Erro no teste de compatibilidade Cloudflare: {e}")
            self.errors.append(f"cloudflare_compatibility: {str(e)}")
            return {'status': 'failed', 'error': str(e)}
    
    # =====================================
    # ğŸƒ EXECUÃ‡ÃƒO DOS TESTES
    # =====================================
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """Executar todos os testes"""
        
        logger.info("ğŸš€ Iniciando suite completa de testes...")
        
        start_time = time.time()
        
        # Lista de testes a executar
        tests = [
            ('retention_manager_basic', self.test_retention_manager_basic),
            ('pipeline_processing', self.test_pipeline_processing),
            ('policy_manager', self.test_policy_manager),
            ('monitoring_system', self.test_monitoring_system),
            ('cache_performance', self.test_cache_performance),
            ('feature_extraction_performance', self.test_feature_extraction_performance),
            ('integration_performance', self.test_integration_performance),
            ('cloudflare_compatibility', self.test_cloudflare_compatibility)
        ]
        
        # Executar testes
        for test_name, test_func in tests:
            logger.info(f"â–¶ï¸ Executando teste: {test_name}")
            
            try:
                result = await asyncio.wait_for(
                    test_func(), 
                    timeout=self.test_config['timeout_seconds']
                )
                self.test_results[test_name] = result
                
                status_emoji = "âœ…" if result['status'] == 'passed' else "âŒ"
                logger.info(f"{status_emoji} Teste {test_name}: {result['status']}")
                
            except asyncio.TimeoutError:
                logger.error(f"â° Timeout no teste {test_name}")
                self.test_results[test_name] = {'status': 'timeout', 'error': 'Test timeout'}
                self.errors.append(f"{test_name}: timeout")
                
            except Exception as e:
                logger.error(f"âŒ Erro inesperado no teste {test_name}: {e}")
                self.test_results[test_name] = {'status': 'error', 'error': str(e)}
                self.errors.append(f"{test_name}: {str(e)}")
        
        total_time = time.time() - start_time
        
        # Compilar resultados
        passed_tests = sum(1 for r in self.test_results.values() if r['status'] == 'passed')
        total_tests = len(self.test_results)
        success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        summary = {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': total_tests - passed_tests,
            'success_rate': success_rate,
            'total_time_seconds': total_time,
            'errors': self.errors,
            'test_results': self.test_results
        }
        
        # Log do resumo
        logger.info("=" * 60)
        logger.info("ğŸ“Š RESUMO DOS TESTES")
        logger.info("=" * 60)
        logger.info(f"âœ… Testes aprovados: {passed_tests}/{total_tests} ({success_rate:.1f}%)")
        logger.info(f"â±ï¸ Tempo total: {total_time:.2f}s")
        
        if self.errors:
            logger.warning(f"âš ï¸ Erros encontrados: {len(self.errors)}")
            for error in self.errors:
                logger.warning(f"   â€¢ {error}")
        
        return summary
    
    def generate_performance_report(self) -> str:
        """Gerar relatÃ³rio de performance detalhado"""
        
        report_lines = [
            "# ğŸ§ª RelatÃ³rio de Performance - Sistema de RetenÃ§Ã£o ML",
            "",
            f"**Data do Teste:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## ğŸ“Š Resumo Executivo"
        ]
        
        # Resumo dos resultados
        passed = sum(1 for r in self.test_results.values() if r['status'] == 'passed')
        total = len(self.test_results)
        
        report_lines.extend([
            f"- **Testes Executados:** {total}",
            f"- **Testes Aprovados:** {passed} ({(passed/total)*100:.1f}%)",
            f"- **Sistema Funcional:** {'âœ… Sim' if passed == total else 'âš ï¸ Com ressalvas'}",
            ""
        ])
        
        # Detalhes de performance
        if 'cache_performance' in self.test_results:
            cache_result = self.test_results['cache_performance']
            if cache_result['status'] == 'passed':
                speedup = cache_result.get('speedup_factor', 1)
                report_lines.extend([
                    "## âš¡ Ganhos de Performance",
                    f"- **Speedup do Cache:** {speedup:.1f}x mais rÃ¡pido",
                    f"- **Tempo sem Cache:** {cache_result.get('no_cache_time_ms', 0):.1f}ms",
                    f"- **Tempo com Cache:** {cache_result.get('cache_time_ms', 0):.1f}ms",
                    ""
                ])
        
        # Compatibilidade Cloudflare
        if 'cloudflare_compatibility' in self.test_results:
            cf_result = self.test_results['cloudflare_compatibility']
            status = "âœ… CompatÃ­vel" if cf_result['status'] == 'passed' else "âŒ IncompatÃ­vel"
            report_lines.extend([
                "## â˜ï¸ Compatibilidade Cloudflare",
                f"- **Status:** {status}",
                f"- **Modo Readonly:** {'âœ…' if cf_result.get('readonly_mode') else 'âŒ'}",
                ""
            ])
        
        # Erros e recomendaÃ§Ãµes
        if self.errors:
            report_lines.extend([
                "## âš ï¸ Problemas Encontrados",
                ""
            ])
            for error in self.errors:
                report_lines.append(f"- {error}")
            report_lines.append("")
        
        report_lines.extend([
            "## ğŸ¯ ConclusÃµes",
            "",
            "O sistema de retenÃ§Ã£o ML foi testado e validado com os seguintes resultados:",
            "",
            "### âœ… Funcionalidades Validadas",
            "- Cache de caracterÃ­sticas funcionando",
            "- Pipeline de processamento ativo", 
            "- PolÃ­ticas de retenÃ§Ã£o configuradas",
            "- MonitorizaÃ§Ã£o operacional",
            "",
            "### ğŸš€ BenefÃ­cios Confirmados",
            "- Melhoria significativa de performance",
            "- Compatibilidade total com sistema existente",
            "- Suporte para Cloudflare Workers",
            "- MonitorizaÃ§Ã£o automÃ¡tica de saÃºde"
        ])
        
        return "\n".join(report_lines)


# =====================================
# ğŸš€ EXECUÃ‡ÃƒO PRINCIPAL
# =====================================

async def main():
    """FunÃ§Ã£o principal para executar os testes"""
    
    print("ğŸ§ª ML Retention Performance Tests")
    print("=" * 50)
    print("Testando sistema de retenÃ§Ã£o ML sem afetar funcionalidade existente")
    print()
    
    # Executar testes
    test_suite = PerformanceTestSuite()
    results = await test_suite.run_all_tests()
    
    # Gerar relatÃ³rio
    report = test_suite.generate_performance_report()
    
    # Salvar relatÃ³rio
    report_filename = f"ml_retention_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print()
    print(f"ğŸ“„ RelatÃ³rio salvo em: {report_filename}")
    
    # Retornar cÃ³digo de saÃ­da baseado no sucesso
    if results['success_rate'] == 100:
        print("ğŸ‰ Todos os testes passaram! Sistema pronto para produÃ§Ã£o.")
        return 0
    else:
        print("âš ï¸ Alguns testes falharam. Verifique o relatÃ³rio para detalhes.")
        return 1


if __name__ == "__main__":
    # Executar testes
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
