"""
üîó ML Retention Integration
Integra√ß√£o n√£o-invasiva que otimiza sistema ML existente com cache de reten√ß√£o

COMPATIBILIDADE TOTAL:
- N√£o modifica APIs existentes
- Funciona como middleware transparente  
- Mant√©m Cloudflare Workers funcionando
- Performance boost autom√°tico
"""

import asyncio
import functools
import logging
import time
from typing import Dict, List, Optional, Any, Callable, Union
from datetime import datetime, timedelta

# Imports do sistema existente (n√£o-invasivos)
try:
    from .retention_manager import MLRetentionManager, get_retention_manager
    from .retention_pipeline import MLRetentionPipeline, get_retention_pipeline
    from .retention_policies import MLRetentionPolicyManager, get_policy_manager
    from .ml_model_manager import MLModelManager
    from ..api.ml_endpoints import *
    from ..models.biodiversity_ml_schemas import *
except ImportError:
    # Fallback para desenvolvimento
    import sys
    sys.path.append('../../')

logger = logging.getLogger(__name__)


class MLRetentionIntegrator:
    """
    üîó Integrador de Reten√ß√£o ML
    
    Sistema que intercepta e otimiza opera√ß√µes ML existentes
    de forma completamente transparente.
    """
    
    def __init__(self):
        """Inicializar integrador"""
        
        self.retention_manager = get_retention_manager()
        self.pipeline = get_retention_pipeline()
        self.policy_manager = get_policy_manager()
        
        # Flags de controle
        self.enabled = True
        self.cloudflare_mode = False
        self.performance_mode = True
        
        # M√©tricas de integra√ß√£o
        self.integration_metrics = {
            'queries_intercepted': 0,
            'cache_hits': 0,
            'performance_gains_ms': 0,
            'original_functions_preserved': 0
        }
        
        # Registry de fun√ß√µes originais
        self.original_functions = {}
        
        logger.info("üîó ML Retention Integrator inicializado")
    
    # =====================================
    # üéØ MONKEY PATCHING N√ÉO-INVASIVO
    # =====================================
    
    def integrate_with_ml_endpoints(self):
        """
        Integrar com endpoints ML existentes
        
        Aplica monkey patching para adicionar cache transparente
        """
        try:
            # Importar m√≥dulo de endpoints
            import src.bgapp.api.ml_endpoints as ml_endpoints_module
            
            # Interceptar fun√ß√£o de predi√ß√£o
            if hasattr(ml_endpoints_module, 'make_prediction'):
                self._wrap_prediction_function(ml_endpoints_module, 'make_prediction')
            
            # Interceptar fun√ß√µes de treino
            if hasattr(ml_endpoints_module, 'train_model'):
                self._wrap_training_function(ml_endpoints_module, 'train_model')
            
            # Interceptar fun√ß√µes de dados
            if hasattr(ml_endpoints_module, 'get_study'):
                self._wrap_data_function(ml_endpoints_module, 'get_study')
            
            logger.info("‚úÖ Integra√ß√£o com ML endpoints aplicada")
            
        except ImportError:
            logger.warning("‚ö†Ô∏è M√≥dulo ml_endpoints n√£o encontrado - integra√ß√£o pulada")
        except Exception as e:
            logger.error(f"‚ùå Erro na integra√ß√£o com endpoints: {e}")
    
    def integrate_with_ml_manager(self):
        """
        Integrar com MLModelManager existente
        
        Adiciona cache transparente √†s opera√ß√µes do gestor
        """
        try:
            # Interceptar m√©todos do MLModelManager
            original_predict = MLModelManager.predict
            original_train = MLModelManager.train_model
            original_get_features = getattr(MLModelManager, 'extract_features', None)
            
            # Wrapping com cache
            MLModelManager.predict = self._create_cached_predict(original_predict)
            MLModelManager.train_model = self._create_cached_train(original_train)
            
            if original_get_features:
                MLModelManager.extract_features = self._create_cached_features(original_get_features)
            
            # Salvar originais
            self.original_functions['MLModelManager.predict'] = original_predict
            self.original_functions['MLModelManager.train_model'] = original_train
            if original_get_features:
                self.original_functions['MLModelManager.extract_features'] = original_get_features
            
            logger.info("‚úÖ Integra√ß√£o com MLModelManager aplicada")
            
        except Exception as e:
            logger.error(f"‚ùå Erro na integra√ß√£o com MLModelManager: {e}")
    
    def _wrap_prediction_function(self, module, function_name: str):
        """Wrap fun√ß√£o de predi√ß√£o com cache"""
        original_func = getattr(module, function_name)
        self.original_functions[f"{module.__name__}.{function_name}"] = original_func
        
        @functools.wraps(original_func)
        async def cached_prediction_wrapper(*args, **kwargs):
            if not self.enabled:
                return await original_func(*args, **kwargs)
            
            try:
                # Extrair par√¢metros de predi√ß√£o
                request = args[1] if len(args) > 1 else kwargs.get('request')
                if not request or not hasattr(request, 'model_type'):
                    return await original_func(*args, **kwargs)
                
                model_type = request.model_type
                input_data = request.input_data
                
                # Tentar cache primeiro
                cached_result = await self.retention_manager.get_or_compute_prediction(
                    model_id=model_type,
                    input_data=input_data,
                    predict_func=lambda data: original_func(*args, **kwargs),
                    ttl_hours=6
                )
                
                self.integration_metrics['queries_intercepted'] += 1
                self.integration_metrics['cache_hits'] += 1
                
                return cached_result
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Cache falhou, usando fun√ß√£o original: {e}")
                return await original_func(*args, **kwargs)
        
        setattr(module, function_name, cached_prediction_wrapper)
        self.integration_metrics['original_functions_preserved'] += 1
    
    def _wrap_training_function(self, module, function_name: str):
        """Wrap fun√ß√£o de treino com cache"""
        original_func = getattr(module, function_name)
        self.original_functions[f"{module.__name__}.{function_name}"] = original_func
        
        @functools.wraps(original_func)
        async def cached_training_wrapper(*args, **kwargs):
            if not self.enabled:
                return await original_func(*args, **kwargs)
            
            try:
                # Extrair tipo de modelo
                model_type = args[1] if len(args) > 1 else kwargs.get('model_type', 'unknown')
                data_version = kwargs.get('data_version', 'latest')
                
                # Tentar cache de dados de treino
                cached_data = await self.retention_manager.get_or_prepare_training_data(
                    model_type=model_type,
                    data_version=data_version,
                    prepare_func=lambda **kw: original_func(*args, **kwargs)
                )
                
                self.integration_metrics['queries_intercepted'] += 1
                self.integration_metrics['cache_hits'] += 1
                
                return cached_data
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Cache de treino falhou: {e}")
                return await original_func(*args, **kwargs)
        
        setattr(module, function_name, cached_training_wrapper)
        self.integration_metrics['original_functions_preserved'] += 1
    
    def _wrap_data_function(self, module, function_name: str):
        """Wrap fun√ß√£o de dados com cache"""
        original_func = getattr(module, function_name)
        self.original_functions[f"{module.__name__}.{function_name}"] = original_func
        
        @functools.wraps(original_func)
        async def cached_data_wrapper(*args, **kwargs):
            if not self.enabled:
                return await original_func(*args, **kwargs)
            
            try:
                # Extrair ID do estudo
                study_id = args[1] if len(args) > 1 else kwargs.get('study_id')
                
                if study_id:
                    # Interceptar para pipeline de processamento
                    result = await original_func(*args, **kwargs)
                    
                    # Processar em background se for novo estudo
                    if result and isinstance(result, dict):
                        asyncio.create_task(
                            self.pipeline.process_new_study(result)
                        )
                    
                    self.integration_metrics['queries_intercepted'] += 1
                    return result
                
                return await original_func(*args, **kwargs)
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Processamento de dados falhou: {e}")
                return await original_func(*args, **kwargs)
        
        setattr(module, function_name, cached_data_wrapper)
        self.integration_metrics['original_functions_preserved'] += 1
    
    # =====================================
    # üîß FACTORY METHODS PARA CACHE
    # =====================================
    
    def _create_cached_predict(self, original_predict):
        """Criar vers√£o cacheada da fun√ß√£o de predi√ß√£o"""
        
        @functools.wraps(original_predict)
        async def cached_predict(self_manager, model_type: str, input_data: Dict[str, Any], **kwargs):
            if not self.enabled:
                return await original_predict(self_manager, model_type, input_data, **kwargs)
            
            start_time = time.time()
            
            try:
                # Usar cache de infer√™ncia
                result = await self.retention_manager.get_or_compute_prediction(
                    model_id=model_type,
                    input_data=input_data,
                    predict_func=lambda data: original_predict(self_manager, model_type, input_data, **kwargs),
                    ttl_hours=6
                )
                
                execution_time = (time.time() - start_time) * 1000
                self.integration_metrics['performance_gains_ms'] += execution_time
                self.integration_metrics['cache_hits'] += 1
                
                logger.debug(f"üöÄ Predi√ß√£o cacheada: {model_type} ({execution_time:.1f}ms poupados)")
                
                return result
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Cache de predi√ß√£o falhou: {e}")
                return await original_predict(self_manager, model_type, input_data, **kwargs)
        
        return cached_predict
    
    def _create_cached_train(self, original_train):
        """Criar vers√£o cacheada da fun√ß√£o de treino"""
        
        @functools.wraps(original_train)
        async def cached_train(self_manager, model_type: str, **kwargs):
            if not self.enabled:
                return await original_train(self_manager, model_type, **kwargs)
            
            start_time = time.time()
            
            try:
                data_version = kwargs.get('data_version', 'latest')
                
                # Usar cache de dados de treino
                training_data = await self.retention_manager.get_or_prepare_training_data(
                    model_type=model_type,
                    data_version=data_version,
                    prepare_func=lambda **kw: original_train(self_manager, model_type, **kwargs)
                )
                
                execution_time = (time.time() - start_time) * 1000
                self.integration_metrics['performance_gains_ms'] += execution_time
                self.integration_metrics['cache_hits'] += 1
                
                logger.info(f"üöÄ Treino cacheado: {model_type} ({execution_time:.1f}ms poupados)")
                
                return training_data
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Cache de treino falhou: {e}")
                return await original_train(self_manager, model_type, **kwargs)
        
        return cached_train
    
    def _create_cached_features(self, original_extract):
        """Criar vers√£o cacheada da extra√ß√£o de caracter√≠sticas"""
        
        @functools.wraps(original_extract)
        async def cached_extract_features(self_manager, study_id: str, feature_type: str, **kwargs):
            if not self.enabled:
                return await original_extract(self_manager, study_id, feature_type, **kwargs)
            
            try:
                # Usar cache de caracter√≠sticas
                features = await self.retention_manager.get_or_compute_features(
                    source_data_id=study_id,
                    source_table='biodiversity_studies',
                    feature_type=feature_type,
                    compute_func=lambda sid, **kw: original_extract(self_manager, study_id, feature_type, **kwargs),
                    **kwargs
                )
                
                self.integration_metrics['cache_hits'] += 1
                
                return features
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Cache de caracter√≠sticas falhou: {e}")
                return await original_extract(self_manager, study_id, feature_type, **kwargs)
        
        return cached_extract_features
    
    # =====================================
    # üåê CLOUDFLARE WORKERS INTEGRATION
    # =====================================
    
    def enable_cloudflare_mode(self):
        """Ativar modo compat√≠vel com Cloudflare Workers"""
        self.cloudflare_mode = True
        self.retention_manager.enable_cloudflare_mode()
        
        # Configurar para modo readonly
        self.retention_manager.readonly_mode = True
        
        logger.info("‚òÅÔ∏è Modo Cloudflare ativado para integra√ß√£o")
    
    def create_worker_middleware(self):
        """
        Criar middleware para Cloudflare Workers
        
        Returns:
            Fun√ß√£o middleware que pode ser usada nos workers
        """
        
        def worker_middleware(request_handler):
            """Middleware para otimizar requests em workers"""
            
            async def optimized_handler(request, env, ctx):
                # Verificar se √© request ML
                url = request.url
                
                if '/ml/' in url:
                    # Tentar cache primeiro
                    cache_key = f"worker_{url}_{request.method}"
                    
                    # Cache simples em mem√≥ria para workers
                    if hasattr(env, 'ML_CACHE') and cache_key in env.ML_CACHE:
                        logger.debug(f"‚òÅÔ∏è Worker cache HIT: {cache_key}")
                        return env.ML_CACHE[cache_key]
                
                # Executar handler original
                response = await request_handler(request, env, ctx)
                
                # Cache response se for ML
                if '/ml/' in url and response.status == 200:
                    if not hasattr(env, 'ML_CACHE'):
                        env.ML_CACHE = {}
                    env.ML_CACHE[cache_key] = response
                
                return response
            
            return optimized_handler
        
        return worker_middleware
    
    # =====================================
    # üîÑ GEST√ÉO DE INTEGRA√á√ÉO
    # =====================================
    
    def enable_integration(self):
        """Ativar integra√ß√£o completa"""
        self.enabled = True
        
        # Aplicar todas as integra√ß√µes
        self.integrate_with_ml_endpoints()
        self.integrate_with_ml_manager()
        
        # Iniciar pipeline em background
        asyncio.create_task(self.pipeline.start_background_processing())
        
        logger.info("üöÄ Integra√ß√£o ML completa ativada")
    
    def disable_integration(self):
        """Desativar integra√ß√£o (restaurar fun√ß√µes originais)"""
        self.enabled = False
        
        # Restaurar fun√ß√µes originais
        self.restore_original_functions()
        
        # Parar pipeline
        asyncio.create_task(self.pipeline.stop_background_processing())
        
        logger.info("‚èπÔ∏è Integra√ß√£o ML desativada")
    
    def restore_original_functions(self):
        """Restaurar fun√ß√µes originais (rollback)"""
        
        for func_path, original_func in self.original_functions.items():
            try:
                # Parse do caminho da fun√ß√£o
                parts = func_path.split('.')
                
                if len(parts) >= 2:
                    module_name = '.'.join(parts[:-1])
                    function_name = parts[-1]
                    
                    # Importar m√≥dulo e restaurar fun√ß√£o
                    if 'MLModelManager' in module_name:
                        setattr(MLModelManager, function_name, original_func)
                    else:
                        # Para m√≥dulos de endpoints
                        import importlib
from bgapp.core.logger import logger
                        module = importlib.import_module(module_name)
                        setattr(module, function_name, original_func)
                
                logger.debug(f"‚úÖ Fun√ß√£o restaurada: {func_path}")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro restaurando {func_path}: {e}")
        
        self.original_functions.clear()
    
    # =====================================
    # üìä MONITORING & METRICS
    # =====================================
    
    def get_integration_metrics(self) -> Dict[str, Any]:
        """Obter m√©tricas de integra√ß√£o"""
        
        # Combinar com m√©tricas do retention manager
        retention_stats = self.retention_manager.get_lightweight_stats()
        
        return {
            'integration_enabled': self.enabled,
            'cloudflare_mode': self.cloudflare_mode,
            'queries_intercepted': self.integration_metrics['queries_intercepted'],
            'cache_hits': self.integration_metrics['cache_hits'],
            'cache_hit_ratio': self.integration_metrics['cache_hits'] / max(1, self.integration_metrics['queries_intercepted']),
            'performance_gains_ms': self.integration_metrics['performance_gains_ms'],
            'original_functions_preserved': self.integration_metrics['original_functions_preserved'],
            'retention_manager_stats': retention_stats,
            'pipeline_metrics': self.pipeline.get_pipeline_metrics() if hasattr(self.pipeline, 'get_pipeline_metrics') else {}
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """Verificar sa√∫de da integra√ß√£o"""
        
        health_status = {
            'integration_active': self.enabled,
            'retention_manager': 'healthy',
            'pipeline': 'healthy',
            'policy_manager': 'healthy',
            'cache_performance': 'good',
            'issues': []
        }
        
        try:
            # Verificar retention manager
            stats = self.retention_manager.get_lightweight_stats()
            if stats['hit_ratio'] < 0.1:
                health_status['cache_performance'] = 'poor'
                health_status['issues'].append('Low cache hit ratio')
            
            # Verificar pipeline
            if hasattr(self.pipeline, 'background_task'):
                if not self.pipeline.background_task or self.pipeline.background_task.done():
                    health_status['pipeline'] = 'stopped'
                    health_status['issues'].append('Background pipeline not running')
            
            # Verificar policy manager
            if hasattr(self.policy_manager, 'scheduler_task'):
                if not self.policy_manager.scheduler_task or self.policy_manager.scheduler_task.done():
                    health_status['policy_manager'] = 'stopped'
                    health_status['issues'].append('Policy scheduler not running')
            
        except Exception as e:
            health_status['issues'].append(f"Health check error: {str(e)}")
        
        health_status['overall_status'] = 'healthy' if not health_status['issues'] else 'degraded'
        
        return health_status


# =====================================
# üöÄ GLOBAL INTEGRATOR
# =====================================

# Inst√¢ncia global
integrator = None

def get_retention_integrator() -> MLRetentionIntegrator:
    """Obter inst√¢ncia global do integrador"""
    global integrator
    
    if integrator is None:
        integrator = MLRetentionIntegrator()
    
    return integrator


def initialize_ml_retention_system(
    enable_integration: bool = True,
    cloudflare_mode: bool = False,
    auto_start: bool = True
) -> MLRetentionIntegrator:
    """
    Inicializar sistema completo de reten√ß√£o ML
    
    Esta √© a fun√ß√£o principal para ativar todo o sistema.
    
    Args:
        enable_integration: Ativar integra√ß√£o com sistema existente
        cloudflare_mode: Modo compat√≠vel com Cloudflare Workers
        auto_start: Iniciar servi√ßos automaticamente
    
    Returns:
        Inst√¢ncia do integrador configurada
    """
    
    integrator = get_retention_integrator()
    
    if cloudflare_mode:
        integrator.enable_cloudflare_mode()
    
    if enable_integration:
        integrator.enable_integration()
    
    if auto_start:
        # Iniciar todos os servi√ßos
        asyncio.create_task(integrator.pipeline.start_background_processing())
        asyncio.create_task(integrator.policy_manager.start_scheduler())
    
    logger.info("üöÄ Sistema de reten√ß√£o ML inicializado completamente")
    
    return integrator


# Fun√ß√µes de conveni√™ncia para uso direto
def enable_ml_retention():
    """Ativar sistema de reten√ß√£o ML (fun√ß√£o simples)"""
    return initialize_ml_retention_system(
        enable_integration=True,
        cloudflare_mode=False,
        auto_start=True
    )


def enable_ml_retention_cloudflare():
    """Ativar sistema de reten√ß√£o ML para Cloudflare"""
    return initialize_ml_retention_system(
        enable_integration=True,
        cloudflare_mode=True,
        auto_start=False  # Workers n√£o precisam de background tasks
    )


if __name__ == "__main__":
    # Teste b√°sico
    async def test_integration():
        integrator = initialize_ml_retention_system(
            enable_integration=False,  # N√£o modificar sistema em teste
            auto_start=False
        )
        
        metrics = integrator.get_integration_metrics()
        health = await integrator.health_check()
        
        logger.info("üìä Integration Metrics:", metrics)
        logger.info("ü©∫ Health Check:", health)
    
    asyncio.run(test_integration())
